{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4202351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a2bcb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/workspace'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfca8082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/opt/workspace/AB_NYC_2019.csv')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path('./AB_NYC_2019.csv').resolve()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919b5fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'host_id', 'host_name', 'neighbourhood_group', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']\n",
      "['2539', 'Clean & quiet apt home by the park', '2787', 'John', 'Brooklyn', 'Kensington', '40.64749', '-73.97237', 'Private room', '149', '1', '9', '2018-10-19', '0.21', '6', '365']\n",
      "['2595', 'Skylit Midtown Castle', '2845', 'Jennifer', 'Manhattan', 'Midtown', '40.75362', '-73.98377', 'Entire home/apt', '225', '1', '45', '2019-05-21', '0.38', '2', '355']\n",
      "['3647', 'THE VILLAGE OF HARLEM....NEW YORK !', '4632', 'Elisabeth', 'Manhattan', 'Harlem', '40.80902', '-73.9419', 'Private room', '150', '3', '0', '', '', '1', '365']\n",
      "['3831', 'Cozy Entire Floor of Brownstone', '4869', 'LisaRoxanne', 'Brooklyn', 'Clinton Hill', '40.68514', '-73.95976', 'Entire home/apt', '89', '1', '270', '2019-07-05', '4.64', '1', '194']\n",
      "['5022', 'Entire Apt: Spacious Studio/Loft by central park', '7192', 'Laura', 'Manhattan', 'East Harlem', '40.79851', '-73.94399', 'Entire home/apt', '80', '10', '9', '2018-11-19', '0.10', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "with open(path, 'r', newline='', encoding='utf-8') as f:\n",
    "    \n",
    "    csv_file = csv.reader(f)\n",
    "    \n",
    "    row = 0\n",
    "    for line in csv_file:\n",
    "        row += 1\n",
    "        print(line)\n",
    "        if row > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c0a2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_price(el: str) ->tuple([int]):\n",
    "    try:\n",
    "        price = int(el['price'])\n",
    "        return (1, price, 0)\n",
    "    except:\n",
    "        return (0, 0, 0) \n",
    "\n",
    "\n",
    "def reducer(data1, data2) -> tuple([int, float]):\n",
    "    if  data2[0] > 0:\n",
    "        n = data1[0]\n",
    "        n += data2[0]\n",
    "        delta = data2[1] - data1[1]\n",
    "        mean = data1[1] + delta / n\n",
    "        M2 = data1[2] + delta * (data2[1] - mean)\n",
    "        return n, mean, M2\n",
    "    else:    \n",
    "        return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98954a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 48895, mean = 152.72068718682823, std_dev = 240.15171391941672, D = 57672.845698433375\n",
      "n = 48895, mean = 152.72068718682823, std_dev = 240.15171391941672, D = 57672.845698433375\n",
      "n = 48895, mean = 152.72068718682823, std_dev = 240.15171391941672, D = 57672.845698433375\n",
      "n = 48895, mean = 152.72068718682823, std_dev = 240.15171391941672, D = 57672.845698433375\n",
      "n = 48895, mean = 152.72068718682823, std_dev = 240.15171391941672, D = 57672.845698433375\n",
      "n = 48895, mean = 152.72068718682823, std_dev = 240.15171391941672, D = 57672.845698433375\n",
      "n = 48895, mean = 152.72068718682823, std_dev = 240.15171391941672, D = 57672.845698433375\n",
      "n = 48895, mean = 152.72068718682823, std_dev = 240.15171391941672, D = 57672.845698433375\n",
      "565 ms ± 1.32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "with open(path, 'r', newline='', encoding='utf-8') as f:\n",
    "    csv_file = csv.DictReader(f)\n",
    "    n = 0\n",
    "    mapped_data = map(mapper_price, csv_file)\n",
    "#     print(list(mapped_data)[:10])\n",
    "    n, mean, M2 = reduce(reducer, mapped_data)\n",
    "    print(f'n = {n}, mean = {mean}, std_dev = {(M2 / n) ** (1/2)}, D = {M2/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5bae753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e681a1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "23/12/30 12:09:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_json,col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"pyspark-notebook\")\\\n",
    "        .master(\"spark://spark-master:7077\")\\\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3a587a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv(path=\"./AB_NYC_2019.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd510a45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'name',\n",
       " 'host_id',\n",
       " 'host_name',\n",
       " 'neighbourhood_group',\n",
       " 'neighbourhood',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'room_type',\n",
       " 'price',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'last_review',\n",
       " 'reviews_per_month',\n",
       " 'calculated_host_listings_count',\n",
       " 'availability_365']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c48b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_spark.select(df_spark.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e155ed4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'> <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_spark), type(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "041c2adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/30 12:24:02 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    }
   ],
   "source": [
    "df_spark.write.saveAsTable('ab_nyc_2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7d2046",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
